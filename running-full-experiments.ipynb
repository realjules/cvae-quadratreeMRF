{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":323641,"sourceType":"datasetVersion","datasetId":136436},{"sourceId":277914,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":238030,"modelId":259698},{"sourceId":277925,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":238040,"modelId":259708}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NEW CODE","metadata":{}},{"cell_type":"code","source":"# CRFNet-RS: Experiment Runner for Different Ground Truth Sparsity Levels\n\n# Cell 1: Setup Environment\nimport os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport random\nfrom tqdm.notebook import tqdm\nfrom glob import glob\nimport time\nfrom datetime import datetime\nfrom skimage import io\n\n# Set random seeds for reproducibility\nrandom.seed(42)\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Clone the repository\n!git clone https://github.com/Ayana-Inria/CRFNet-RS.git\nsys.path.append('./CRFNet-RS')\n\n# Install dependencies\n!pip install -r ./CRFNet-RS/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:15:53.506857Z","iopub.execute_input":"2025-03-24T10:15:53.507184Z","iopub.status.idle":"2025-03-24T10:16:03.600631Z","shell.execute_reply.started":"2025-03-24T10:15:53.507162Z","shell.execute_reply":"2025-03-24T10:16:03.599567Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'CRFNet-RS' already exists and is not an empty directory.\nCollecting matplotlib==3.7.0 (from -r ./CRFNet-RS/requirements.txt (line 1))\n  Using cached matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nCollecting numpy==1.21.0 (from -r ./CRFNet-RS/requirements.txt (line 2))\n  Using cached numpy-1.21.0.zip (10.3 MB)\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting opencv-python==4.7.0.68 (from -r ./CRFNet-RS/requirements.txt (line 3))\n  Using cached opencv_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu111 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu111\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Fix the unet import issue\nwith open('./CRFNet-RS/main.py', 'r') as file:\n    content = file.read()\n\n# Replace the incorrect import\nfixed_content = content.replace('from net.unet import *', '# from net.unet import *')\n\nwith open('./CRFNet-RS/main.py', 'w') as file:\n    file.write(fixed_content)\n\n# Fix utils_network.py indentation\n!sed -i '89s/^/        /' /kaggle/working/CRFNet-RS/utils/utils_network.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:03.602123Z","iopub.execute_input":"2025-03-24T10:16:03.602449Z","iopub.status.idle":"2025-03-24T10:16:03.737583Z","shell.execute_reply.started":"2025-03-24T10:16:03.602416Z","shell.execute_reply":"2025-03-24T10:16:03.736677Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Import Required Modules\nfrom dataset.dataset import ISPRS_dataset\nfrom utils.utils_dataset import convert_from_color, convert_to_color, disk, conn_comp\nfrom utils.utils import metrics, sliding_window, count_sliding_window, grouper\nfrom utils.export_result import set_output_location, export_results\nfrom net.net import CRFNet\nfrom net.loss import CrossEntropy2d\nfrom skimage import io\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport cv2\nfrom utils.utils_network import compute_class_weight\nfrom net.test_network import test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:03.739421Z","iopub.execute_input":"2025-03-24T10:16:03.739656Z","iopub.status.idle":"2025-03-24T10:16:03.744647Z","shell.execute_reply.started":"2025-03-24T10:16:03.739636Z","shell.execute_reply":"2025-03-24T10:16:03.744027Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Configuration parameters\nWINDOW_SIZE = (256, 256)  # Patch size\nSTRIDE = 32  # Stride for testing inference\nIN_CHANNELS = 3  # Number of input channels (RGB/IRRG)\nBATCH_SIZE = 10  # Mini-batch size - KEEP THIS AS IN PAPER\nEPOCHS = 30  # Number of training epochs - MATCH PAPER VALUE\nSAVE_EPOCH = 10  # Save model every N epochs\nBASE_LR = 0.01  # Base learning rate - MATCH PAPER VALUE\nWEIGHT_DECAY = 0.0005  # Weight decay for optimizer - MATCH PAPER VALUE\nERO_DISK_SIZE = 8  # Size of erosion disk for ground truth processing\n\n# Dataset parameters\nDATASET_TYPE = \"Vaihingen\"  # Fixed to Vaihingen for this experiment\nDATA_ROOT = \"/kaggle/input/potsdamvaihingen/\"  # Input data path\nOUTPUT_ROOT = \"/kaggle/working/\"  # Working directory for outputs\nWORKING_DATA_ROOT = \"/kaggle/working/data\"  # Working directory for data processing\n\n# Set paths based on dataset\ntrain_ids = ['1', '3', '23', '26', '7', '11', '13', '28', '17', '32', '34', '37']\ntest_ids = ['5', '15', '21', '30']\n\n# Data file paths\nDATA_FILES = f\"{DATA_ROOT}/ISPRS_semantic_labeling_Vaihingen/top/top_mosaic_09cm_area{{}}.tif\"\nLABEL_FILES = f\"{DATA_ROOT}/ISPRS_semantic_labeling_Vaihingen/gts_for_participants/top_mosaic_09cm_area{{}}.tif\"\n\n# Class labels\nLABELS = [\"roads\", \"buildings\", \"low veg.\", \"trees\", \"cars\", \"clutter\"]\nN_CLASSES = len(LABELS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:03.745664Z","iopub.execute_input":"2025-03-24T10:16:03.745924Z","iopub.status.idle":"2025-03-24T10:16:03.759101Z","shell.execute_reply.started":"2025-03-24T10:16:03.745904Z","shell.execute_reply":"2025-03-24T10:16:03.758374Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Create necessary directories\nos.makedirs(OUTPUT_ROOT, exist_ok=True)\nos.makedirs(WORKING_DATA_ROOT, exist_ok=True)\nos.makedirs(f\"{WORKING_DATA_ROOT}/top\", exist_ok=True)\nos.makedirs(f\"{WORKING_DATA_ROOT}/gt\", exist_ok=True)\nos.makedirs(f\"{WORKING_DATA_ROOT}/gt_eroded\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:03.759905Z","iopub.execute_input":"2025-03-24T10:16:03.760151Z","iopub.status.idle":"2025-03-24T10:16:03.775262Z","shell.execute_reply.started":"2025-03-24T10:16:03.760121Z","shell.execute_reply":"2025-03-24T10:16:03.774567Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Function to train model\ndef train_model(net, optimizer, scheduler, train_loader, epochs, save_epoch, weights, output_path):\n    \"\"\"\n    Train the CRFNet model with proper loss function implementation\n    \"\"\"\n    # Initialize loss tracking\n    max_iterations = epochs * len(train_loader)\n    losses = np.zeros(max_iterations)\n    mean_losses = np.zeros(max_iterations)\n    iter_ = 0\n    \n    # Training loop\n    for e in tqdm(range(1, epochs + 1), desc=\"Epochs\"):\n        # Set model to training mode\n        net.train()\n        \n        # Process each batch\n        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {e}\", leave=False)):\n            # Process targets for multi-scale supervision\n            target_np = target.data.cpu().numpy()\n            target_np = np.transpose(target_np, [1, 2, 0])\n            \n            # Create multi-scale targets for different decoder outputs\n            target3 = np.transpose(cv2.resize(target_np, dsize=(128, 128), interpolation=cv2.INTER_NEAREST), [2, 0, 1])\n            target2 = np.transpose(cv2.resize(target_np, dsize=(64, 64), interpolation=cv2.INTER_NEAREST), [2, 0, 1])\n            target1 = np.transpose(cv2.resize(target_np, dsize=(32, 32), interpolation=cv2.INTER_NEAREST), [2, 0, 1])\n            target_np = np.transpose(target_np, [2, 0, 1])\n            \n            # Move data to device\n            if torch.cuda.is_available():\n                data = Variable(data.cuda())\n                target = Variable(torch.from_numpy(target_np).cuda())\n                target1_tensor = Variable(torch.from_numpy(target1).type(torch.LongTensor).cuda())\n                target2_tensor = Variable(torch.from_numpy(target2).type(torch.LongTensor).cuda())\n                target3_tensor = Variable(torch.from_numpy(target3).type(torch.LongTensor).cuda())\n            else:\n                data = Variable(data)\n                target = Variable(torch.from_numpy(target_np))\n                target1_tensor = Variable(torch.from_numpy(target1).type(torch.LongTensor))\n                target2_tensor = Variable(torch.from_numpy(target2).type(torch.LongTensor))\n                target3_tensor = Variable(torch.from_numpy(target3).type(torch.LongTensor))\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass - get all outputs\n            output, out_fc, out_neigh, _ = net(data)\n            \n            # Calculate multi-scale losses as described in the paper\n            # 1. Unary loss from main output\n            loss = CrossEntropy2d(output, target, weight=weights)\n            \n            # 2. Multi-scale supervision losses\n            if torch.cuda.is_available():\n                loss_fc1 = CrossEntropy2d(out_fc[0], target1_tensor, weight=compute_class_weight(target1).cuda())\n                loss_fc2 = CrossEntropy2d(out_fc[1], target2_tensor, weight=compute_class_weight(target2).cuda())\n                loss_fc3 = CrossEntropy2d(out_fc[2], target3_tensor, weight=compute_class_weight(target3).cuda())\n            else:\n                loss_fc1 = CrossEntropy2d(out_fc[0], target1_tensor, weight=compute_class_weight(target1))\n                loss_fc2 = CrossEntropy2d(out_fc[1], target2_tensor, weight=compute_class_weight(target2))\n                loss_fc3 = CrossEntropy2d(out_fc[2], target3_tensor, weight=compute_class_weight(target3))\n            \n            # 3. Pairwise potential loss from neighborhood layer\n            pairwise_loss = CrossEntropy2d(out_neigh, target, weight=weights)\n            \n            # 4. Combine losses exactly as in the paper\n            # Unary loss is average of multi-scale losses plus pairwise loss\n            total_loss = (loss + loss_fc1 + loss_fc2 + loss_fc3) / 4 + pairwise_loss\n            \n            # Backward pass and optimization\n            total_loss.backward()\n            optimizer.step()\n            \n            # Record loss\n            losses[iter_] = total_loss.item()\n            mean_losses[iter_] = np.mean(losses[max(0, iter_-100):iter_+1])\n            \n            # Display progress every 100 iterations\n            if iter_ % 100 == 0:\n                # Visualize results if desired\n                # (Visualization code removed for brevity and stability)\n                print(f'Epoch {e}/{epochs} [{batch_idx}/{len(train_loader)} ({100*batch_idx/len(train_loader):.0f}%)] Loss: {total_loss.item():.4f}')\n            \n            iter_ += 1\n        \n        # Update learning rate\n        scheduler.step()\n        \n        # Save model checkpoint\n        if e % save_epoch == 0:\n            model_path = f'{output_path}/model_epoch{e}.pth'\n            torch.save(net.state_dict(), model_path)\n            print(f\"Model saved to {model_path}\")\n    \n    # Save final model\n    final_model_path = f'{output_path}/model_final.pth'\n    torch.save(net.state_dict(), final_model_path)\n    print(f\"Final model saved to {final_model_path}\")\n    return final_model_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:03.775931Z","iopub.execute_input":"2025-03-24T10:16:03.776222Z","iopub.status.idle":"2025-03-24T10:16:03.794704Z","shell.execute_reply.started":"2025-03-24T10:16:03.776192Z","shell.execute_reply":"2025-03-24T10:16:03.794033Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Function to test the model\ndef test_model(net, test_ids, data_files, label_files, labels, stride, batch_size, window_size, output_path=None):\n    \"\"\"\n    Test the model on the provided test data\n    \"\"\"\n    # Load test data\n    test_images = [1/255 * np.asarray(io.imread(data_files.format(id)), dtype='float32') for id in test_ids]\n    test_labels = [np.asarray(io.imread(label_files.format(id)), dtype='uint8') for id in test_ids]\n    eroded_labels = [convert_from_color(label) for label in test_labels]\n    \n    # Run the test\n    acc, all_preds, all_gts = test(\n        net, test_ids, test_images, test_labels, eroded_labels, \n        labels, stride, batch_size, window_size=window_size, all=True\n    )\n    \n    # Export results\n    if output_path:\n        title = \"Quantitative results for CRFNet testing\"\n        export_results(\n            all_preds, all_gts, \n            os.path.dirname(output_path), os.path.basename(output_path),\n            confusionMat=True,\n            prodAccuracy=True,\n            averageAccuracy=True,\n            kappaCoeff=True,\n            title=title\n        )\n        \n        # Save prediction images\n        for pred, tile_id in zip(all_preds, test_ids):\n            img = convert_to_color(pred)\n            io.imsave(f\"{output_path}/segmentation_result_area{tile_id}.png\", img)\n    \n    return acc, all_preds, all_gts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:03.795545Z","iopub.execute_input":"2025-03-24T10:16:03.795846Z","iopub.status.idle":"2025-03-24T10:16:03.812587Z","shell.execute_reply.started":"2025-03-24T10:16:03.795818Z","shell.execute_reply":"2025-03-24T10:16:03.811799Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Run experiments for all three GT percentages: full, 30%, and 10%\ndef run_experiment(gt_type, gt_percentage=None):\n    \"\"\"\n    Run a complete training and testing cycle for a specific ground truth configuration\n    \n    Parameters:\n    gt_type - Type of ground truth: 'full', 'conncomp', 'ero'\n    gt_percentage - For 'conncomp' or 'ero', the percentage of labeled pixels (10 or 30)\n    \"\"\"\n    # Create experiment name\n    if gt_type == 'full':\n        experiment_name = f\"CRFNet_{DATASET_TYPE}_full_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n    else:\n        experiment_name = f\"CRFNet_{DATASET_TYPE}_{gt_type}_{gt_percentage}pct_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n    \n    # Create experiment directory\n    os.makedirs(f\"{OUTPUT_ROOT}/{experiment_name}\", exist_ok=True)\n    \n    print(f\"Starting experiment: {experiment_name}\")\n    print(f\"Dataset: {DATASET_TYPE}\")\n    print(f\"Ground Truth Type: {gt_type}\")\n    if gt_type != 'full':\n        print(f\"Sparse GT percentage: {gt_percentage}%\")\n    \n    # Initialize the CRFNet model with the correct neighborhood system (8-connected/second-order)\n    net = CRFNet(n_channels=IN_CHANNELS, n_classes=N_CLASSES, bilinear=True)\n    \n    # Setup optimizer and learning rate scheduler as described in the paper\n    optimizer = optim.SGD(net.parameters(), lr=BASE_LR, momentum=0.9, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [25, 35, 45], gamma=0.1)\n    \n    # Move model to GPU if available\n    if torch.cuda.is_available():\n        net.cuda()\n        weights = torch.ones(N_CLASSES).cuda()\n    else:\n        weights = torch.ones(N_CLASSES)\n    \n    # Create dataset with correct ground truth processing\n    train_set = ISPRS_dataset(\n        ids=train_ids,\n        ids_type='TRAIN',\n        gt_type=gt_type,\n        gt_modification=disk(ERO_DISK_SIZE) if gt_type != 'full' else None,\n        data_files=DATA_FILES,\n        label_files=LABEL_FILES,\n        window_size=WINDOW_SIZE,\n        cache=True,\n        augmentation=True,\n        sparse_percentage=gt_percentage if gt_type != 'full' else None\n    )\n    \n    # Create data loader\n    train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE)\n    print(f\"Created training loader with approximately {len(train_set) // BATCH_SIZE} batches per epoch\")\n    \n    # Train the model\n    print(\"Starting model training...\")\n    model_path = train_model(\n        net=net,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        train_loader=train_loader,\n        epochs=EPOCHS,\n        save_epoch=SAVE_EPOCH,\n        weights=weights,\n        output_path=f\"{OUTPUT_ROOT}/{experiment_name}\"\n    )\n    print(f\"Training completed! Model saved to {model_path}\")\n    \n    # Test the model\n    print(\"Starting model testing...\")\n    accuracy, all_preds, all_gts = test_model(\n        net=net,\n        test_ids=test_ids,\n        data_files=DATA_FILES,\n        label_files=LABEL_FILES,\n        labels=LABELS,\n        stride=STRIDE,\n        batch_size=BATCH_SIZE,\n        window_size=WINDOW_SIZE,\n        output_path=f\"{OUTPUT_ROOT}/{experiment_name}\"\n    )\n    \n    print(f\"Testing completed with overall accuracy: {accuracy:.2f}%\")\n    \n    # Create a visualization of all test results\n    n_images = len(test_ids)\n    fig, axes = plt.subplots(n_images, 3, figsize=(15, 5*n_images))\n    \n    for i, (id, pred, gt) in enumerate(zip(test_ids, all_preds, all_gts)):\n        # Load original image\n        img = io.imread(DATA_FILES.format(id))\n        \n        # Display original image, ground truth, and prediction\n        if n_images > 1:\n            axes[i, 0].imshow(img)\n            axes[i, 0].set_title(f'Area {id} - Original')\n            axes[i, 1].imshow(convert_to_color(gt))\n            axes[i, 1].set_title('Ground Truth')\n            axes[i, 2].imshow(convert_to_color(pred))\n            axes[i, 2].set_title('Prediction')\n        else:\n            axes[0].imshow(img)\n            axes[0].set_title(f'Area {id} - Original')\n            axes[1].imshow(convert_to_color(gt))\n            axes[1].set_title('Ground Truth')\n            axes[2].imshow(convert_to_color(pred))\n            axes[2].set_title('Prediction')\n    \n    plt.tight_layout()\n    plt.savefig(f\"{OUTPUT_ROOT}/{experiment_name}/all_results.png\", dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"Results visualization saved to {OUTPUT_ROOT}/{experiment_name}/all_results.png\")\n    \n    # Return experiment results\n    return {\n        'experiment_name': experiment_name,\n        'accuracy': accuracy,\n        'model_path': model_path,\n        'predictions': all_preds\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:03.814125Z","iopub.execute_input":"2025-03-24T10:16:03.814317Z","iopub.status.idle":"2025-03-24T10:16:03.834076Z","shell.execute_reply.started":"2025-03-24T10:16:03.814300Z","shell.execute_reply":"2025-03-24T10:16:03.833218Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Run all three experiments (full, 30%, 10%)\nresults = {}\n\n# 1. Full ground truth experiment\nprint(\"=\" * 80)\nprint(\"RUNNING EXPERIMENT 1/3: FULL GROUND TRUTH\")\nprint(\"=\" * 80)\nresults['full'] = run_experiment(gt_type='full')\n\n# 2. 30% ground truth experiment\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RUNNING EXPERIMENT 2/3: 30% GROUND TRUTH\")\nprint(\"=\" * 80)\nresults['30pct'] = run_experiment(gt_type='conncomp', gt_percentage=30)\n\n# 3. 10% ground truth experiment\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RUNNING EXPERIMENT 3/3: 10% GROUND TRUTH\")\nprint(\"=\" * 80)\nresults['10pct'] = run_experiment(gt_type='conncomp', gt_percentage=10)\n\n# Print summary of results\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EXPERIMENT RESULTS SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"Full GT: {results['full']['accuracy']:.2f}%\")\nprint(f\"30% GT: {results['30pct']['accuracy']:.2f}%\")\nprint(f\"10% GT: {results['10pct']['accuracy']:.2f}%\")\nprint(\"=\" * 80)\n\n# Create a bar chart comparing the results\nplt.figure(figsize=(10, 6))\nlabels = ['Full GT', '30% GT', '10% GT']\nvalues = [results['full']['accuracy'], results['30pct']['accuracy'], results['10pct']['accuracy']]\nplt.bar(labels, values, color=['green', 'blue', 'orange'])\nplt.axhline(y=85, color='r', linestyle='--', label='Paper 30% Result')\nplt.axhline(y=81, color='purple', linestyle='--', label='Paper 10% Result')\nplt.axhline(y=90, color='green', linestyle='--', label='Paper Full Result')\nplt.ylabel('Overall Accuracy (%)')\nplt.title('CRFNet Performance on Vaihingen Dataset')\nplt.legend()\nplt.grid(axis='y', alpha=0.3)\nplt.savefig(f\"{OUTPUT_ROOT}/accuracy_comparison.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"All experiments completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:03.834933Z","iopub.execute_input":"2025-03-24T10:16:03.835253Z","iopub.status.idle":"2025-03-24T10:16:04.056997Z","shell.execute_reply.started":"2025-03-24T10:16:03.835182Z","shell.execute_reply":"2025-03-24T10:16:04.055928Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nRUNNING EXPERIMENT 1/3: FULL GROUND TRUTH\n================================================================================\nStarting experiment: CRFNet_Vaihingen_full_20250324_101603\nDataset: Vaihingen\nGround Truth Type: full\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-fef801426fc8>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RUNNING EXPERIMENT 1/3: FULL GROUND TRUTH\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 2. 30% ground truth experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-91fd36a28309>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(gt_type, gt_percentage)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Create dataset with correct ground truth processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     train_set = ISPRS_dataset(\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mids_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TRAIN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ISPRS_dataset.__init__() got an unexpected keyword argument 'sparse_percentage'"],"ename":"TypeError","evalue":"ISPRS_dataset.__init__() got an unexpected keyword argument 'sparse_percentage'","output_type":"error"}],"execution_count":18}]}